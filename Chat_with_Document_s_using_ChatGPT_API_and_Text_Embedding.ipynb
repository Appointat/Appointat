{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1CXFb-SvAxleQi7bupxwCa9WCrtZmQnFb",
      "authorship_tag": "ABX9TyOBjVzClFHzziE5JyZCyxVM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Appointat/Appointat/blob/main/Chat_with_Document_s_using_ChatGPT_API_and_Text_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation of packages"
      ],
      "metadata": {
        "id": "ezgiv0WbYYMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install chromadb\n",
        "!pip install langchain\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "vkKmsqN-YdDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Python Packages"
      ],
      "metadata": {
        "id": "3-hiK0fyarvt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TJrP3usXYl0",
        "outputId": "390df39d-3db1-4994-96d9-3e332c2604a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python:  3.9.16\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import platform\n",
        "import textwrap\n",
        "\n",
        "import openai\n",
        "import chromadb\n",
        "import langchain\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ChatVectorDBChain\n",
        "from langchain.document_loaders import GutenbergLoader\n",
        "\n",
        "print('Python: ', platform.python_version())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive on Colab"
      ],
      "metadata": {
        "id": "1NjniZC9YZEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSeq13ffYVfh",
        "outputId": "7ccf4a21-272c-455b-8bc8-75b9b62e17f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI API Key"
      ],
      "metadata": {
        "id": "Lw_8ouylbJU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = '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'"
      ],
      "metadata": {
        "id": "OWH1yPo_bMLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure Chroma"
      ],
      "metadata": {
        "id": "Sygktu5eYEs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory=\"/content/drive/My Drive/Colab Notebooks/chroma/romeo\""
      ],
      "metadata": {
        "id": "6oyASQR6a3Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert Document to Embedding"
      ],
      "metadata": {
        "id": "Vo92dxtUYE-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gutenberg(url):\n",
        "    loader = GutenbergLoader(url)\n",
        "    data = loader.load()\n",
        "    return data"
      ],
      "metadata": {
        "id": "jiINbdENa3gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the Romeo and Juliet text data from Project Gutenberg\n",
        "romeoandjuliet_data = get_gutenberg('https://www.gutenberg.org/cache/epub/1513/pg1513.txt')\n",
        "\n",
        "# Initializing a TokenTextSplitter object to split the text into chunks of 1000 tokens with 0 token overlap\n",
        "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "\n",
        "# Splitting the Romeo and Juliet text into chunks using the TokenTextSplitter object\n",
        "romeoandjuliet_doc = text_splitter.split_documents(romeoandjuliet_data)\n",
        "\n",
        "# Initializing an OpenAIEmbeddings object for word embeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Generating Chroma vectors from the text chunks using the OpenAIEmbeddings object and persisting them to disk\n",
        "vectordb = Chroma.from_documents(romeoandjuliet_doc, embeddings, persist_directory=persist_directory)\n",
        "vectordb.persist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JwgArJYa8AU",
        "outputId": "f61ef04a-39db-4a11-edc7-7dfa55451839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Chroma:Logger created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Chroma using direct local API.\n",
            "loaded in 124 embeddings\n",
            "loaded in 1 collections\n",
            "collection with name langchain already exists, returning existing collection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Chroma:Index saved to /content/drive/My Drive/Colab Notebooks/chroma/romeo/index/index.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Persisting DB to disk, putting it in the save folder /content/drive/My Drive/Colab Notebooks/chroma/romeo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure LangChain QA"
      ],
      "metadata": {
        "id": "7tfc8C5ha31r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "romeoandjuliet_qa = ChatVectorDBChain.from_llm(OpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"), vectordb, return_source_documents=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqdQKtFba_Gc",
        "outputId": "141892eb-1e2e-4a3b-b8d8-624bc63c7bda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/langchain/llms/openai.py:169: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/langchain/llms/openai.py:608: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Have Romeo and Juliet spent the night together? Provide a verbose answer, referencing passages from the book.\"\n",
        "chat_history = ''\n",
        "result = romeoandjuliet_qa({\"question\": query, \"chat_history\": chat_history})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9qDaY0IBGkY",
        "outputId": "1e0940d4-f12e-4c2e-c447-9bc27e8ae311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Chroma:time to pre process our knn query: 3.337860107421875e-06\n",
            "DEBUG:Chroma:time to run knn query: 0.00040435791015625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"source_documents\"]"
      ],
      "metadata": {
        "id": "gcl3UbvqB4H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "ifoLr3S3CEyq",
        "outputId": "1826c99f-db86-4d0b-fa87-7da4242ef024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"It is unclear from the given context whether Romeo and Juliet have spent the night together. The passages provided focus on Romeo's banishment and his despair over being separated from Juliet. The Nurse comes to Friar Lawrence seeking Romeo's whereabouts, and when she finds him, she reports that Juliet is weeping and calling out for Romeo. However, there is no mention of whether they spent the night together or not.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def markdown_to_python(markdown_text):\n",
        "    # Escape quotes and backslashes in the input\n",
        "    escaped_input = markdown_text.replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"\\\\'\")\n",
        "\n",
        "    # Generate the Python string\n",
        "    python_string = f\"'{escaped_input}'\"\n",
        "\n",
        "    return python_string"
      ],
      "metadata": {
        "id": "s1m_iQS199dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "markdown_text = \"Generating questions and answers from the book is a straightforward process. To assess the accuracy of the results, I will be comparing the answers with those from SparkNotes. > *SparkNotes editors.* [“Romeo and Juliet” SparkNotes.com](https://www.sparknotes.com/shakespeare/romeojuliet/key-questions-and-answers/), *SparkNotes LLC, 2005* >\"\n",
        "query = markdown_to_python(markdown_text);\n",
        "result = romeoandjuliet_qa({\"question\": query, \"chat_history\": chat_history})\n",
        "chat_history = chat_history + result[\"answer\"]\n",
        "result[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "5J6B5FTC80nv",
        "outputId": "0e17f730-5426-4a50-8687-87edc3193b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Chroma:time to pre process our knn query: 3.814697265625e-06\n",
            "DEBUG:Chroma:time to run knn query: 0.0003476142883300781\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is not a question, it is a statement.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# restart the conversation\n",
        "chat_history = [(\"hello\", \"hello\")]"
      ],
      "metadata": {
        "id": "wRK1QcjmIfom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "markdown_text = \"我得到了一个字符串，写python代码，需要将字符串自动识别，输出array[str]。中文回答\"\n",
        "\n",
        "query = markdown_to_python(markdown_text)\n",
        "result = romeoandjuliet_qa({\"question\": query, \"chat_history\": chat_history})\n",
        "chat_history = chat_history + [(query, result[\"answer\"])]\n",
        "formatted_history = \"\\n\".join([f\"Question: {q}\\nAnswer: {a}\" for q, a in chat_history])\n",
        "wrapped_history = textwrap.fill(formatted_history, width=120)\n",
        "print(wrapped_history + \"\\n\")\n",
        "result[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "D0jlxG4m_fLd",
        "outputId": "3ba92d99-f0c3-48a9-a562-71d133555c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a67a65fe9188>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmarkdown_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"我得到了一个字符串，写python代码，需要将字符串自动识别，输出array[str]。中文回答\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarkdown_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkdown_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mromeoandjuliet_qa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"chat_history\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchat_history\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mchat_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat_history\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'markdown_to_python' is not defined"
          ]
        }
      ]
    }
  ]
}